{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: 10-Fold Cross Validation Performance of *QRF* model\n",
    "\n",
    "Performing a cross-validation similar to our evaluation method in this scenario implies:\n",
    "\n",
    "* *Temporal and Spatial Splitting:* To accurately replicate the evaluation dataset, the splits must be designed so that no dates or stations in the evaluation fold appear in the training set.\n",
    "\n",
    "<img src=\"../../images/eval.png\" alt=\"Experiment Diagram\" style=\"width:50%;\" />\n",
    "\n",
    "* *Data Scaling:* The evaluation dataset is scaled to balance the contribution of each station, ensuring that errors from stations with high water streamflow do not overshadow those from stations with lower streamflow.\n",
    "* *Prediction Intervals:* A non-standard evaluation approach based on log-likelihood is used to account for prediction intervals.\n",
    "\n",
    "\n",
    "\n",
    "This notebook addresses these question by employing a custom `SpatioTemporalSplit` class for folding, a pipeline for proper scaling, and a custom `custom_log_likelihood` scorer for performance evaluation.\n",
    "\n",
    "### 1. Data Import and Setup\n",
    "\n",
    "Imports necessary libraries, sets up environment paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third-party imports\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Append project root to sys.path for local imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', '..')))\n",
    "\n",
    "# Local application imports\n",
    "from src.utils.model import get_station_stats, custom_log_likelihood\n",
    "from src.utils.SpatioTemporalSplit import SpatioTemporalSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines constants :\n",
    "* INPUT_DIR must be the same as the one defined in *00 Preprocessing/Feature Engineering*.\n",
    "* MODEL_DIR is the directory where the exploration models will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../../data/input/\"\n",
    "MODEL_DIR = \"../../../../models/exploration/\"\n",
    "\n",
    "SEED = 42 \n",
    "ALPHA = 0.1\n",
    "WEEK_TO_PREDICT=1 \n",
    "\n",
    "# columns to dropÂ : target at different horizon, station_code, and features removed from Feature Selection\n",
    "TO_DROP = [\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "ds_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "train_data = ds_train.copy()\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data = train_data.loc[:, ~train_data.columns.duplicated()]\n",
    "ds_train = ds_train.set_index(\"ObsDate\")\n",
    "y_train = train_data[f\"water_flow_week{WEEK_TO_PREDICT}\"]\n",
    "cv_data = train_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model preparation\n",
    "\n",
    "Compute station statistics (usefull for scalling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_stats = get_station_stats(\n",
    "    y_train.to_numpy(),\n",
    "    train_data[\"station_code\"].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Pipeline to keep track of the station code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = TO_DROP\n",
    "cols_to_drop += [\"ObsDate\"]\n",
    "\n",
    "predictor_cols = [col for col in cv_data.columns if col not in cols_to_drop]\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('select', 'passthrough', predictor_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "qrf_week1 = RandomForestQuantileRegressor(n_estimators=10, max_depth=10, min_samples_leaf=10)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', qrf_week1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the log likelihood scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = partial(custom_log_likelihood,\n",
    "                 cv_data=cv_data,\n",
    "                 station_stats=station_stats,\n",
    "                 alpha=ALPHA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the SpatioTemporal Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = SpatioTemporalSplit(\n",
    "    n_splits=10,\n",
    "    date_col='ObsDate',\n",
    "    station_col='station_code',\n",
    "    temporal_frac=0.75,\n",
    "    spatial_frac=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    pipeline,    # our pipeline estimator\n",
    "    cv_data,     # full data with all columns needed for splitting\n",
    "    y_train,     # target variable\n",
    "    cv=cv,       # custom spatio-temporal splitter\n",
    "    scoring=scorer # custom scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10-Fold CV performance (per fold log-likelihood):\", scores)\n",
    "print(\"Average log-likelihood:\", np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
